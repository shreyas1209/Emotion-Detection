# -*- coding: utf-8 -*-
"""Emotion_train.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FFUucyjV19uCsXKOCMRPpQ9OgrBk1lvm
"""

import torch.nn.functional as F
import time 
import random
from torch import nn
import torch
import numpy as np
import matplotlib.pyplot as plt
from torch import nn,optim
from torch.utils.data import Dataset, DataLoader

from google.colab import drive
drive.mount('/content/gdrive')

import sys
sys.path.append('/content/gdrive/MyDrive/Emotion Detector')

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

from Emotion_models import *

def check_accuracy(model, train_loader):
  num_correct = 0
  num_samples = 0
  model.eval()
  model = model.to(device)
  
  with torch.no_grad():
    for DATA  in train_loader:
      data = DATA[0]
      data = data.to(device)
      classes = DATA[1]
      classes = classes.to(device)
      scores = model(data)
      predictions = scores.argmax(1)
      num_correct += sum((predictions == classes))
      num_samples += predictions.size(0)

    return float(num_correct)/float(num_samples)

def train(train_loader, val_loader,epochs,model,loss_function,optimizer): 
  
  if torch.cuda.is_available(): torch.cuda.empty_cache()
  model = model.to(device = device)
  history = {"train_accuracy": [], "val_accuracy": [], "loss":[], "lr": []}

  optimizer = optimizer
  scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=1e-3, max_lr=0.05,cycle_momentum=False, step_size_up=5, mode="triangular2")
  loss_func = loss_function

  
  for epoch in range(epochs):
    model.train()
    loss_train=0
    start_time = time.process_time()
  
    for batch_no , DATA in enumerate(train_loader):
      #print(batch_no)
      data = DATA[0]
      data = data.to(device)
      classes = DATA[1]
      classes = classes.to(device)

      optimizer.zero_grad()
      
      output = model(data)
      loss = loss_func(output,classes)
      
      loss.backward()
     
      nn.utils.clip_grad_norm_(model.parameters(), max_norm = 1) #Grad clip for Resnet
      loss_train += loss.item() 
      optimizer.step()
    
    history["lr"].append(optimizer.param_groups[0]["lr"])
        
    scheduler.step()

    history["train_accuracy"].append(check_accuracy(model, train_loader))
    history["val_accuracy"].append(check_accuracy(model, val_loader))
    history["loss"].append(loss_train/(len(train_loader)))
        
    end_time = time.process_time()
    exec_time = end_time-start_time

    print('Epoch ({}/{}),Training loss : {:.4f}, Time: {:.2f},Last lr: {:.5f} train_accuracy:{:.4f}, val_accuracy:{:.4f}'.format(
          epoch+1, epochs, history["loss"][-1], exec_time, history["lr"][-1], history["train_accuracy"][-1], history["val_accuracy"][-1]))

  return model, history

#random generated data


